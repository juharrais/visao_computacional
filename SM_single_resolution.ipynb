{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SM_single_resolution.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbKStiz719RAbREJBraA9S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juharrais/visao_computacional/blob/main/SM_single_resolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb_ZPSTB3DRy"
      },
      "outputs": [],
      "source": [
        "# Nome: Juliana Arrais\n",
        "# Segmentation Models Doc\n",
        "# https://github.com/qubvel/segmentation_models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Libs & Configs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9Lytgbr3PN6",
        "outputId": "2783fbf0-6b1f-47a1-e11b-6ed7d64d3710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lib installation\n",
        "!pip3 install -r \"gdrive/Shareddrives/Nuvens/requirements_keras/requirements.txt\" --use-deprecated=legacy-resolver -q"
      ],
      "metadata": {
        "id": "m6kZHiYL3PTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7715fffd-94cd-4815-b7df-f9ced4c39da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 129 kB 12.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 98 kB 5.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 727 kB 51.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 45.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.7 MB 19.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 283 kB 50.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.7 MB 30.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 88 kB 6.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 147 kB 50.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 402 kB 44.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 214 kB 49.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 31.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 42.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 361.9 MB 30 kB/s \n",
            "\u001b[K     |████████████████████████████████| 11.6 MB 23.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 44.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 81 kB 10.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 54.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 48.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 41.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 61 kB 24 kB/s \n",
            "\u001b[K     |████████████████████████████████| 146 kB 50.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 46.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 51.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 38.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 45.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 53.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 36.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 121 kB 51.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 630 kB 45.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 38.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 303 kB 48.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 82 kB 1.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 52.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 38.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 36.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 183 kB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 273 kB 49.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 108 kB 42.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 82 kB 1.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 178 kB 52.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 44.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 23.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 471 kB 38.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 41.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 52.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 38.2 MB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 41.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 40 kB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 47.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 28.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 231 kB 50.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 41 kB 437 kB/s \n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 26.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 362 kB/s \n",
            "\u001b[K     |████████████████████████████████| 126 kB 53.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 41.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.7 MB 9.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 43.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 746 kB 45.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 869 kB 45.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 512 kB 47.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 47.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 106 kB 50.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 227 kB 41.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 204 kB 47.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 240 kB 48.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 39.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 37.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 119 kB 50.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 187 kB 47.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 26.5 MB 59.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 19.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 47.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 285 kB 46.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 41.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 45.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 89 kB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 24.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 792 kB/s \n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 44.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 25.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 781 kB 40.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 454.3 MB 19 kB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 43.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 408 kB 51.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.8 MB 38.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 165 kB 46.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.9 kB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 100 kB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 37.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 282 kB 45.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 172.6 MB 53 kB/s \n",
            "\u001b[?25h  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for Bottleneck (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for earthengine-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gdown (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kapre (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for multitasking (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openpyxl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pandocfilters (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyrsistent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-louvain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for retrying (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rpy2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for scs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import CSVLogger\n",
        "\n",
        "# from keras.utils import normalize\n",
        "import os\n",
        "import pandas as pd\n",
        "from requests import get\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "\n",
        "\n",
        "import segmentation_models as sm\n",
        "sm.set_framework('tf.keras')\n",
        "from segmentation_models.utils import set_trainable\n",
        "import albumentations as A\n",
        "\n",
        "\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "j4TvNan23PWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SIZE_X = 1024\n",
        "SIZE_Y = SIZE_X\n",
        "n_classes = 5\n",
        "IMG_CHANNELS = 3 # Grayscale = 1 || RGB = 3\n",
        "\n",
        "ALL_CLASSES = [\n",
        "                \"Arvore\",                  #0\n",
        "                \"Estratocumuliformes\",     #1\n",
        "                \"Estratiformes\",           #2\n",
        "                \"Cirriformes\",             #3\n",
        "                \"Cumuliformes\",            #4\n",
        "                \n",
        "            ]\n",
        "\n",
        "CLASSES = [\n",
        "                \"Arvore\",                  #0\n",
        "                \"Estratocumuliformes\",     #1\n",
        "                \"Estratiformes\",           #2\n",
        "                \"Cirriformes\",             #3\n",
        "                \"Cumuliformes\",            #4\n",
        "            ]\n",
        "\n",
        "seed = 99\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "DATA_DIR = '/content/gdrive/Shareddrives/Nuvens/datasets/Albedo(merged classes)_001 - 997 images'\n",
        "MODELS_DIR = 'gdrive/Shareddrives/Nuvens/trained_models/keras/'\n",
        "\n",
        "notebook_filename = get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n",
        "MODELS_DIR = MODELS_DIR + notebook_filename+'/'\n",
        "\n",
        "RESULTS_DIR = MODELS_DIR + 'results/'\n",
        "\n",
        "x_train_dir = os.path.join(DATA_DIR, 'images')\n",
        "y_train_dir = os.path.join(DATA_DIR, 'labels')\n",
        "\n",
        "x_valid_dir = os.path.join(DATA_DIR, 'valid_images')\n",
        "y_valid_dir = os.path.join(DATA_DIR, 'valid_labels')\n",
        "\n",
        "x_test_dir = os.path.join(DATA_DIR, 'test_images')\n",
        "y_test_dir = os.path.join(DATA_DIR, 'test_labels')"
      ],
      "metadata": {
        "id": "gHmThFqi3PYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset class\n",
        "# classes for data loading and preprocessing\n",
        "class Dataset:\n",
        "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
        "    \n",
        "    Args:\n",
        "        images_dir (str): path to images folder\n",
        "        masks_dir (str): path to segmentation masks folder\n",
        "        class_values (list): values of classes to extract from segmentation mask\n",
        "        augmentation (albumentations.Compose): data transfromation pipeline \n",
        "            (e.g. flip, scale, etc.)\n",
        "        preprocessing (albumentations.Compose): data preprocessing \n",
        "            (e.g. noralization, shape manipulation, etc.)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    CLASSES = [\n",
        "                \"Arvore\",                  #0\n",
        "                \"Estratocumuliformes\",     #1\n",
        "                \"Estratiformes\",           #2\n",
        "                \"Cirriformes\",             #3\n",
        "                \"Cumuliformes\",            #4\n",
        "            ]\n",
        "    \n",
        "    def __init__(\n",
        "            self, \n",
        "            images_dir, \n",
        "            masks_dir, \n",
        "            classes=None, \n",
        "            augmentation=None, \n",
        "            preprocessing=None,\n",
        "            SIZE_X=SIZE_X,\n",
        "            SIZE_Y=SIZE_Y\n",
        "    ):\n",
        "        self.ids = sorted(os.listdir(images_dir))\n",
        "        self.ids_masks = sorted(os.listdir(masks_dir))\n",
        "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
        "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids_masks]\n",
        "        \n",
        "        # convert str names to class values on masks\n",
        "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
        "        \n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "        self.SIZE_X = SIZE_X\n",
        "        self.SIZE_Y = SIZE_Y\n",
        "\n",
        "        self.all_images = self._read_all_images()\n",
        "        self.all_masks = self._read_all_masks()\n",
        "    \n",
        "    def _read_all_images(self):\n",
        "        resized_images = []\n",
        "        for current_img in self.images_fps:\n",
        "            img = cv2.imread(current_img)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
        "            resized_images.append(img)\n",
        "        return resized_images\n",
        "\n",
        "    def _read_all_masks(self):\n",
        "        resized_masks = []\n",
        "        for current_img in self.masks_fps:\n",
        "            mask = cv2.imread(current_img, 0)\n",
        "            mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)\n",
        "            resized_masks.append(mask)\n",
        "        return resized_masks\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read data\n",
        "        if len(self.all_images) > 0:\n",
        "            image = self.all_images[i]\n",
        "        else:\n",
        "            image = cv2.imread(self.images_fps[i])\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if len(self.all_masks) > 0:\n",
        "            mask = self.all_masks[i]\n",
        "        else:\n",
        "            mask = cv2.imread(self.masks_fps[i], 0)\n",
        "        \n",
        "        # extract certain classes from mask (e.g. cumulus, stratus)\n",
        "        masks = [(mask == v) for v in self.class_values]\n",
        "        mask = np.stack(masks, axis=-1).astype('float')\n",
        "        \n",
        "        # add background if mask is not binary\n",
        "        if mask.shape[-1] != 1:\n",
        "            background = 1 - mask.sum(axis=-1, keepdims=True)\n",
        "            mask = np.concatenate((mask, background), axis=-1)\n",
        "        \n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            \n",
        "        return image, mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ],
      "metadata": {
        "id": "lJ0lknNs3Pah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function for data visualization    \n",
        "def denormalize(x):\n",
        "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
        "    x_max = np.percentile(x, 98)\n",
        "    x_min = np.percentile(x, 2)    \n",
        "    x = (x - x_min) / (x_max - x_min)\n",
        "    x = x.clip(0, 1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "adoyFXfE3Pc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataloder(keras.utils.Sequence):\n",
        "    \"\"\"Load data from dataset and form batches\n",
        "    \n",
        "    Args:\n",
        "        dataset: instance of Dataset class for image loading and preprocessing.\n",
        "        batch_size: Integet number of images in batch.\n",
        "        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(dataset))\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # collect batch data\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        "        \n",
        "        # transpose list of lists\n",
        "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
        "        \n",
        "        return tuple(batch)\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
        "        return len(self.indexes) // self.batch_size\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n",
        "        if self.shuffle:\n",
        "            self.indexes = np.random.permutation(self.indexes)"
      ],
      "metadata": {
        "id": "BGRBvt723Pe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function for data visualization\n",
        "def visualize(cmap=None, img_id=-1, dir=None, figsize=(18, 8), **images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        plt.imshow(image, cmap=cmap)\n",
        "\n",
        "        if cmap:\n",
        "            plt.clim(0, n_classes-1)\n",
        "            clb = plt.colorbar(shrink=0.55)\n",
        "            clb.set_ticks(range(n_classes))\n",
        "            clb.ax.tick_params(labelsize=10)\n",
        "            clb.ax.set_yticklabels(ALL_CLASSES)\n",
        "            clb.ax.set_visible(i == 2)\n",
        "    # plt.show()\n",
        "    image_name = 'Experiment #'+str(img_id)+'.png'\n",
        "    plt.savefig(dir+image_name)"
      ],
      "metadata": {
        "id": "BbeQK9383PhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data augmentation\n",
        "def round_clip_0_1(x, **kwargs):\n",
        "    return x.round().clip(0, 1)\n",
        "\n",
        "# define heavy augmentations\n",
        "def get_training_augmentation(resize_to=(SIZE_X,SIZE_Y)):\n",
        "    train_transform = [\n",
        "\n",
        "        A.HorizontalFlip(p=1),\n",
        "\n",
        "\n",
        "        A.OneOf(\n",
        "            [\n",
        "                A.CLAHE(p=1),\n",
        "                A.RandomBrightnessContrast(p=1),\n",
        "                A.RandomGamma(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "\n",
        "      \n",
        "\n",
        "        A.OneOf(\n",
        "            [\n",
        "                A.RandomBrightnessContrast(p=1),\n",
        "                A.HueSaturationValue(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "        A.Lambda(mask=round_clip_0_1)\n",
        "    ]\n",
        "    return A.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
        "    test_transform = [\n",
        "        A.PadIfNeeded(SIZE_X, SIZE_Y)\n",
        "    ]\n",
        "    return A.Compose(test_transform)\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    \"\"\"Construct preprocessing transform\n",
        "    \n",
        "    Args:\n",
        "        preprocessing_fn (callbale): data normalization function \n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    _transform = [\n",
        "        A.Lambda(image=preprocessing_fn),\n",
        "    ]\n",
        "    return A.Compose(_transform)"
      ],
      "metadata": {
        "id": "W44Y71JY3PjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# # Visualize the data\n",
        "# # \"Arvore\": [1],\n",
        "# # \"Estratocumuliformes\": [2],\n",
        "# # \"Estratiformes\": [3],\n",
        "# # \"Cirriformes\": [4],\n",
        "# # \"Cumuliformes\": [5],\n",
        "\n",
        "\n",
        "\n",
        "# classes = [\n",
        "#             \"Arvore\",                   #0\n",
        "#             \"Estratocumuliformes\",      #1\n",
        "#             \"Estratiformes\",            #2\n",
        "#             \"Cirriformes\",              #3\n",
        "#             \"Cumuliformes\",             #4\n",
        "#         ]\n",
        "# dataset = Dataset(x_train_dir, y_train_dir, classes=classes)\n",
        "\n",
        "# image, mask = dataset[0] # get some sample\n",
        "# visualize(\n",
        "#     image=image, \n",
        "#     Arvore=mask[..., 0].squeeze(),\n",
        "#     # Estratocumuliformes=mask[..., 1].squeeze(),\n",
        "#     # Estratiformes=mask[..., 2].squeeze(),\n",
        "#     # Cirriformes=mask[..., 3].squeeze(),\n",
        "#     # Cumuliformes=mask[..., 4].squeeze(),\n",
        "#     \n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "# # \"Arvore\": [1],\n",
        "# # \"Estratocumuliformes\": [2],\n",
        "# # \"Estratiformes\": [3],\n",
        "# # \"Cirriformes\": [4],\n",
        "# # \"Cumuliformes\": [5],\n",
        "\n",
        "\n",
        "\n",
        "# classes = [\n",
        "#             \"Arvore\",                   #0\n",
        "#             \"Estratocumuliformes\",      #1\n",
        "#             \"Estratiformes\",            #2\n",
        "#             \"Cirriformes\",              #3\n",
        "#             \"Cumuliformes\",             #4\n",
        "#         ]\n",
        "\n",
        "# # Lets look at augmented data we have\n",
        "# dataset = Dataset(x_train_dir, y_train_dir, classes=classes, augmentation=get_training_augmentation())\n",
        "\n",
        "# image, mask = dataset[0] # get some sample\n",
        "# visualize(\n",
        "#     image=image, \n",
        "#     Arvore=mask[..., 0].squeeze(),\n",
        "#     # Estratocumuliformes=mask[..., 1].squeeze(),\n",
        "#     # Estratiformes=mask[..., 2].squeeze(),\n",
        "#     # Cirriformes=mask[..., 3].squeeze(),\n",
        "#     # Cumuliformes=mask[..., 4].squeeze(),\n",
        "#    \n",
        "# )"
      ],
      "metadata": {
        "id": "5oVfNvn4Awwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IoU Class Metric\n",
        "class ClassIoU(keras.metrics.MeanIoU):\n",
        "    \"\"\"Computes the class-specific Intersection-Over-Union metric.\n",
        "\n",
        "    IOU is defined as follows:\n",
        "    IOU = true_positive / (true_positive + false_positive + false_negative).\n",
        "    The predictions are accumulated in a confusion matrix, weighted by\n",
        "    `sample_weight` and the metric is then calculated from it.\n",
        "\n",
        "    If `sample_weight` is `None`, weights default to 1.\n",
        "    Use `sample_weight` of 0 to mask values.\n",
        "\n",
        "    Args:\n",
        "    class_idx: The index of the the class of interest\n",
        "    one_hot: Indicates if the input is a one_hot vector as in CategoricalCrossentropy or if the class indices\n",
        "        are used as in SparseCategoricalCrossentropy or MeanIoU\n",
        "    num_classes: The possible number of labels the prediction task can have.\n",
        "        This value must be provided, since a confusion matrix of dimension =\n",
        "        [num_classes, num_classes] will be allocated.\n",
        "    name: (Optional) string name of the metric instance.\n",
        "    dtype: (Optional) data type of the metric result.\n",
        "    \"\"\"\n",
        "    def __init__(self, class_idx, one_hot, num_classes, name=None, dtype=None):\n",
        "        super().__init__(num_classes, name, dtype)\n",
        "        self.one_hot = one_hot\n",
        "        self.class_idx = class_idx\n",
        "\n",
        "    def result(self):\n",
        "        sum_over_row = tf.cast(\n",
        "            tf.reduce_sum(self.total_cm, axis=0), dtype=self._dtype)\n",
        "        sum_over_col = tf.cast(\n",
        "            tf.reduce_sum(self.total_cm, axis=1), dtype=self._dtype)\n",
        "        true_positives = tf.cast(\n",
        "            tf.linalg.diag_part(self.total_cm), dtype=self._dtype)\n",
        "\n",
        "        # sum_over_row + sum_over_col =\n",
        "        #     2 * true_positives + false_positives + false_negatives.\n",
        "        denominator = sum_over_row[self.class_idx] + sum_over_col[self.class_idx] \\\n",
        "            - true_positives[self.class_idx]\n",
        "\n",
        "        # The mean is only computed over classes that appear in the\n",
        "        # label or prediction tensor. If the denominator is 0, we need to\n",
        "        # ignore the class.\n",
        "        num_valid_entries = tf.reduce_sum(\n",
        "            tf.cast(tf.not_equal(denominator, 0), dtype=self._dtype))\n",
        "\n",
        "        iou = tf.math.divide_no_nan(true_positives[self.class_idx], denominator)\n",
        "\n",
        "        return tf.math.divide_no_nan(\n",
        "            tf.reduce_sum(iou, name='mean_iou'), num_valid_entries)\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        if self.one_hot:\n",
        "            return super().update_state(tf.argmax(y_true, axis=-1), tf.argmax(y_pred, axis=-1), sample_weight)\n",
        "        else:\n",
        "            return super().update_state(y_true, y_pred, sample_weight)"
      ],
      "metadata": {
        "id": "gTcE0K1y4lv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32 \n",
        "BACKBONE = 'resnet18'\n",
        "preprocess_input = sm.get_preprocessing(BACKBONE)"
      ],
      "metadata": {
        "id": "UGnPQ5ky4l35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataloaders\n",
        "# Dataset for train images\n",
        "\n",
        "train_dataset = Dataset(\n",
        "    x_train_dir, \n",
        "    y_train_dir, \n",
        "    classes=CLASSES, \n",
        "    augmentation=get_training_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocess_input),\n",
        ")\n",
        "\n",
        "# Dataset for validation images\n",
        "valid_dataset = Dataset(\n",
        "    x_valid_dir, \n",
        "    y_valid_dir, \n",
        "    classes=CLASSES, \n",
        "    augmentation=get_validation_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocess_input),\n",
        ")\n",
        "\n",
        "train_dataloader = Dataloder(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_dataloader = Dataloder(valid_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "print(train_dataloader[0][0].shape)\n",
        "print(valid_dataloader[0][1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "77tVZEJC4l6f",
        "outputId": "86292f54-4dac-4d2d-9a27-c9af028e92e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-1995b6aee65b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0maugmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_training_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpreprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-7d631d684058>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, images_dir, masks_dir, classes, augmentation, preprocessing, SIZE_X, SIZE_Y)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# convert str names to class values on masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-7d631d684058>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# convert str names to class values on masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'arvore' is not in list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 250\n",
        "\n",
        "#### Activation Function ####\n",
        "n_classes = len(CLASSES) + 1\n",
        "activation = 'softmax'\n",
        "\n",
        "#### Create Model ####\n",
        "model = sm.Linknet(BACKBONE, classes=n_classes, activation=activation, encoder_freeze=True)\n",
        "model._name = 'Linknet' # Alterar de acordo com o modelo acima\n",
        "\n",
        "\n",
        "optim = keras.optimizers.Adam()\n",
        "\n",
        "#### Loss ####\n",
        "total_loss = sm.losses.categorical_focal_dice_loss\n",
        "\n",
        "#### Metrics ####\n",
        "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
        "\n",
        "#### Model Compilation ####\n",
        "model.compile(optim, total_loss, metrics)\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "p1cPnPwN4l8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(MODELS_DIR):\n",
        "    os.mkdir(MODELS_DIR)\n",
        "\n",
        "if not os.path.exists(RESULTS_DIR):\n",
        "    os.mkdir(RESULTS_DIR)\n",
        "\n",
        "model_name = MODELS_DIR + total_loss.name + '__' + model.name + '__' + BACKBONE + '__' + str(EPOCHS) + 'epochs__' +str(SIZE_X)+'x'+str(SIZE_Y)+'.h5'\n",
        "\n",
        "RESULTS_DIR_MODEL = RESULTS_DIR + model_name.split('/')[-1][:-3] + '/'\n",
        "if not os.path.exists(RESULTS_DIR_MODEL):\n",
        "    os.mkdir(RESULTS_DIR_MODEL)\n",
        "\n",
        "if os.path.isfile(model_name):\n",
        "    model.load_weights(model_name)"
      ],
      "metadata": {
        "id": "B_aXaOtj44Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(model_name, save_weights_only=True, save_best_only=True, monitor='val_loss', mode='min'),\n",
        "    keras.callbacks.ReduceLROnPlateau(factor=0.02, patience=2, min_lr=0.00001, monitor='val_loss'),\n",
        "    CSVLogger(RESULTS_DIR_MODEL+'training.log', separator=',', append=True)\n",
        "    \n",
        "]\n",
        "\n",
        "model_name"
      ],
      "metadata": {
        "id": "BtsmrXWL44cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model Pre-training\n",
        "model.fit(\n",
        "    train_dataloader, \n",
        "    steps_per_epoch=len(train_dataloader), \n",
        "    epochs=int(EPOCHS*0.1), \n",
        "    callbacks=callbacks, \n",
        "    validation_data=valid_dataloader, \n",
        "    validation_steps=len(valid_dataloader)\n",
        ")"
      ],
      "metadata": {
        "id": "JIAVCzFe49by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model Training\n",
        "### Unfreezing Layers ###\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "### Training ###\n",
        "history = model.fit(\n",
        "    train_dataloader, \n",
        "    steps_per_epoch=len(train_dataloader), \n",
        "    epochs=EPOCHS, \n",
        "    initial_epoch=0,\n",
        "    callbacks=callbacks, \n",
        "    validation_data=valid_dataloader, \n",
        "    validation_steps=len(valid_dataloader)\n",
        ")"
      ],
      "metadata": {
        "id": "5a4ilgtz49dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Evolution\n",
        "history = pd.read_csv(RESULTS_DIR_MODEL+'training.log', sep=',')\n",
        "\n",
        "#Plot training & validation iou_score values\n",
        "plt.figure(figsize=(30, 10))\n",
        "plt.plot(history['iou_score'])\n",
        "plt.plot(history['val_iou_score'])\n",
        "plt.title('Model iou_score')\n",
        "plt.ylabel('iou_score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "image_name = 'iou_score.png'\n",
        "plt.savefig(RESULTS_DIR_MODEL+image_name)\n",
        "\n",
        "plt.figure(figsize=(30, 10))\n",
        "plt.plot(history['f1-score'])\n",
        "plt.plot(history['val_f1-score'])\n",
        "plt.title('Model f1-score')\n",
        "plt.ylabel('f1-score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "image_name = 'f1_score.png'\n",
        "plt.savefig(RESULTS_DIR_MODEL+image_name)\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(30, 10))\n",
        "plt.plot(history['loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "image_name = 'loss.png'\n",
        "plt.savefig(RESULTS_DIR_MODEL+image_name)\n"
      ],
      "metadata": {
        "id": "0grntsF_49kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Machine Specs & Training History & Model Summary\n",
        "# GPU info\n",
        "from tensorflow.python.client import device_lib\n",
        "with open(RESULTS_DIR_MODEL+\"gpu_specs.txt\", \"w\") as text_file:\n",
        "    text_file.write(str(device_lib.list_local_devices()))\n",
        "\n",
        "# CPU info\n",
        "import cpuinfo\n",
        "with open(RESULTS_DIR_MODEL+\"cpu_specs.txt\", \"w\") as text_file:\n",
        "    text_file.write(str(cpuinfo.get_cpu_info()))\n",
        "\n",
        "# RAM info\n",
        "import psutil\n",
        "with open(RESULTS_DIR_MODEL+\"ram_specs.txt\", \"w\") as text_file:\n",
        "    text_file.write(str(psutil.virtual_memory()))\n",
        "\n",
        "# Model Summary\n",
        "def summary_to_file(s):\n",
        "    with open(RESULTS_DIR_MODEL+'model_summary.txt','a+') as f:\n",
        "        print(s, file=f)\n",
        "model.summary(print_fn = summary_to_file)"
      ],
      "metadata": {
        "id": "xS8Ijwng5Ul-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Dataloader\n",
        "test_dataset = Dataset(\n",
        "    x_test_dir, \n",
        "    y_test_dir, \n",
        "    classes=CLASSES, \n",
        "    augmentation=get_validation_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocess_input),\n",
        ")\n",
        "\n",
        "test_dataloader = Dataloder(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "2RhB7bWN5UoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load best weights only\n",
        "model.load_weights(model_name)\n",
        "\n",
        "#Save overall test results\n",
        "scores = model.evaluate(test_dataloader)\n",
        "\n",
        "with open(RESULTS_DIR_MODEL+\"overall_scores.txt\", \"w\") as text_file:\n",
        "    text_file.write(str(\"Loss: {:.5}\".format(scores[0])))\n",
        "    print(\"Loss: {:.5}\".format(scores[0]))\n",
        "\n",
        "    for metric, value in zip(metrics, scores[1:]):\n",
        "        text_file.write('\\n')\n",
        "        text_file.write(str(\"Mean {}: {:.5}\".format(metric.__name__, value)))\n",
        "        print(\"Mean {}: {:.5}\".format(metric.__name__, value))"
      ],
      "metadata": {
        "id": "rfVQdMG55Uqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sabe images for qualitative analysis\n",
        "# n = 20 # Quantidade de imagens de teste para visualização\n",
        "# ids = random.sample(range(len(test_dataset)), n) # Amostra aleatória de n imagens de teste\n",
        "ids = range(len(test_dataset))\n",
        "img_index = 1\n",
        "\n",
        "for i in ids:\n",
        "    image, gt_mask = test_dataset[i]\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    pr_mask = model.predict(image)\n",
        "    \n",
        "    visualize(\n",
        "        cmap='jet',\n",
        "        img_id=img_index,\n",
        "        dir=RESULTS_DIR_MODEL,\n",
        "        image=denormalize(image.squeeze()),\n",
        "        gt_mask=np.argmax(gt_mask, axis = 2),\n",
        "        pr_mask=np.argmax(pr_mask, axis = 3).squeeze()\n",
        "    )\n",
        "    img_index += 1"
      ],
      "metadata": {
        "id": "k28uzZmw5tka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AWhrOVG45tne"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}